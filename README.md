I was unable to access the repo for llama-2-7b but what I made out of the attached repo is basically it implements the methods described in the paper "Automatic and Universal Prompt Injection Attacks against Large Language Models". The goal is to automatically generate universal prompt injection attacks and manipulate LLMs, regardless of the specific defense mechanisms in place.

Instead of manually crafting attack prompts, attack uses a gradient-based optimization approach to generate adversarial suffixes (attack prompts) that, when appended to user instructions, cause the LLM to output attacker-controlled content.

The first phase is the Attack Generation (universal_prompt_injection.py). In this phase, the script first initializes the attack environment, loads a target language model (Llama2 etc.) and the corresponding tokenizer. It also loads a small set of harmful training samples, which include instructions, inputs, and the attacker's desired outputs. An adversarial suffix is initialized.
Next, the script enters an optimization loop. Over a number of steps, it appends the current adversarial suffix to each training instruction and computes the gradient of the loss with respect to the suffix tokens. This is done using the language model as a differentiable function. The gradients across the batch are aggregated, momentum is applied, and the suffix tokens are updated to maximize the likelihood of the attacker's target output. Utility functions like token_gradients, sample_control, and get_filtered_cands are present.
The goal is to basically produce a universal adversarial suffix that works across many instructions and tasks—not just the ones used in training.
Next its Attack Evaluation (get_responses_universal.py). The purpose here is to test the effectiveness of the generated adversarial suffix on a wide range of tasks, such as summarization, sentiment analysis, and hate detection etc the relevant datasets are present. The script loads the optimized suffix and a set of evaluation instructions. For each instruction, it appends the suffix and queries the language model. The script then checks whether the model’s output contains the attacker's target content (e.g., a specific URL or phrase). This is repeated across different types of injection strategies, including static, semi-dynamic, dynamic, and refuse.
finally its results Checking (check_answers.py). Here, the script reports the success rate across different tasks and models. It loads the results from the evaluation phase and, for each result, checks if the attack target appears in the language model’s output. It then computes and saves the success rate for each configuration.

first tried gpt2 but since it not being an instruction tuned model, the results weren't satisfactory. T5 I found worked pretty well. Outputs of both attached.
